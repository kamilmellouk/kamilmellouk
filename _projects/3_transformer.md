---
layout: page
title: Transformer Architecture for Sentence Compression
description: Implemented a Transformer architecture from scratch in Python and fine-tuned it for the task of sentence compression.
img: assets/img/transformer.png
importance: 3
category: past
giscus_comments: false
---
This project focuses on building a Transformer architecture from scratch using Python and applying it to the task of sentence compression. The transformer was implemented with attention mechanisms, encoder-decoder architecture, and positional embeddings. After successfully building the model, it was fine-tuned using a dataset designed for sentence compression tasks. The model demonstrated strong performance, reducing sentence length while preserving the core meaning and structure, making it efficient for NLP tasks requiring succinct outputs.

<div class="row"> <div class="col-sm mt-3 mt-md-0"> {% include figure.liquid loading="eager" path="assets/img/transformer.png" title="Transformer architecture" class="img-fluid rounded z-depth-1" %} </div> </div> <div class="caption"> Transformer architecture</div>